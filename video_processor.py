# -*- coding: utf-8 -*-
"""
è§†é¢‘å¤„ç†å™¨ - å¤„ç†ç›¸æœºè½¬åŠ¨æ‹æ‘„é™æ€å»ºç­‘çš„è§†é¢‘
è®¡ç®—é™æ€ç‰©ä½“çš„åŠ¨æ€åº¦
"""

import cv2
import numpy as np
import os
import json
import glob
from typing import List, Dict, Optional, Tuple, Union
import matplotlib.pyplot as plt
from tqdm import tqdm
import argparse

from simple_raft import SimpleRAFTPredictor as RAFTPredictor
from static_object_analyzer import StaticObjectDynamicsCalculator
from dynamic_motion_compensation.camera_compensation import CameraCompensator
from unified_dynamics_scorer import UnifiedDynamicsScorer, DynamicsClassifier
from badcase_detector import BadCaseDetector, BadCaseAnalyzer, QualityFilter


class VideoProcessor:
    """è§†é¢‘å¤„ç†å™¨"""
    
    def __init__(self, 
                 raft_model_path: Optional[str] = None,
                 device: str = 'cuda',
                 max_frames: Optional[int] = None,
                 frame_skip: int = 1,
                 enable_visualization: bool = True,
                 enable_camera_compensation: bool = True,
                 camera_compensation_params: Optional[Dict] = None,
                 use_normalized_flow: bool = False,
                 flow_threshold_ratio: float = 0.002):
        """
        åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
        
        Args:
            raft_model_path: RAFTæ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            max_frames: æœ€å¤§å¤„ç†å¸§æ•°
            frame_skip: å¸§è·³è·ƒé—´éš”
            enable_visualization: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–ç»“æœ
            enable_camera_compensation: æ˜¯å¦å¯ç”¨ç›¸æœºè¡¥å¿ï¼ˆé»˜è®¤Trueï¼‰
            camera_compensation_params: ç›¸æœºè¡¥å¿å‚æ•°å­—å…¸
            use_normalized_flow: æ˜¯å¦ä½¿ç”¨åˆ†è¾¨ç‡å½’ä¸€åŒ–ï¼ˆé»˜è®¤Falseä¿æŒå…¼å®¹ï¼‰
            flow_threshold_ratio: å½’ä¸€åŒ–é˜ˆå€¼æ¯”ä¾‹ï¼ˆé»˜è®¤0.002ï¼‰
        """
        self.raft_predictor = RAFTPredictor(raft_model_path, device, method='raft')
        self.max_frames = max_frames
        self.frame_skip = frame_skip
        self.enable_visualization = enable_visualization
        self.enable_camera_compensation = enable_camera_compensation
        self.use_normalized_flow = use_normalized_flow
        
        # åˆå§‹åŒ–ç›¸æœºè¡¥å¿å™¨
        if camera_compensation_params is None:
            camera_compensation_params = {}
        self.camera_compensator = CameraCompensator(**camera_compensation_params) if enable_camera_compensation else None
        
        # åˆå§‹åŒ–åŠ¨æ€åº¦è®¡ç®—å™¨ï¼ˆæ”¯æŒå½’ä¸€åŒ–ï¼‰
        self.dynamics_calculator = StaticObjectDynamicsCalculator(
            use_normalized_flow=use_normalized_flow,
            flow_threshold_ratio=flow_threshold_ratio
        )
        
        # åˆå§‹åŒ–ç»Ÿä¸€åŠ¨æ€åº¦è¯„åˆ†å™¨ï¼ˆä¼ é€’å½’ä¸€åŒ–çŠ¶æ€ï¼‰
        self.unified_scorer = UnifiedDynamicsScorer(
            mode='auto',
            use_normalized_flow=use_normalized_flow
        )
        self.dynamics_classifier = DynamicsClassifier()
        
        # åˆå§‹åŒ–BadCaseæ£€æµ‹å™¨
        self.badcase_detector = BadCaseDetector()
        self.badcase_analyzer = BadCaseAnalyzer()
        
    def load_video(self, video_path: str) -> List[np.ndarray]:
        """åŠ è½½è§†é¢‘å¸§"""
        cap = cv2.VideoCapture(video_path)
        frames = []
        frame_count = 0
        
        print(f"æ­£åœ¨åŠ è½½è§†é¢‘: {video_path}")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            if frame_count % self.frame_skip == 0:
                # è½¬æ¢BGRåˆ°RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame_rgb)
                
                if self.max_frames and len(frames) >= self.max_frames:
                    break
            
            frame_count += 1
        
        cap.release()
        print(f"åŠ è½½å®Œæˆï¼Œå…± {len(frames)} å¸§")
        return frames
    
    def extract_frames_from_images(self, image_dir: str) -> List[np.ndarray]:
        """ä»å›¾åƒç›®å½•åŠ è½½å¸§"""
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            image_files.extend(glob.glob(os.path.join(image_dir, ext)))
        
        image_files.sort()
        frames = []
        
        print(f"æ­£åœ¨ä»ç›®å½•åŠ è½½å›¾åƒ: {image_dir}")
        
        for i, img_path in enumerate(image_files):
            if self.max_frames and i >= self.max_frames:
                break
                
            if i % self.frame_skip == 0:
                img = cv2.imread(img_path)
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                frames.append(img_rgb)
        
        print(f"åŠ è½½å®Œæˆï¼Œå…± {len(frames)} å¸§")
        return frames
    
    def estimate_camera_matrix(self, frame_shape: Tuple[int, int], fov: float = 60.0) -> np.ndarray:
        """ä¼°è®¡ç›¸æœºå†…å‚çŸ©é˜µ"""
        h, w = frame_shape[:2]
        
        # åŸºäºè§†åœºè§’ä¼°è®¡ç„¦è·
        focal_length = w / (2 * np.tan(np.radians(fov / 2)))
        
        # æ„å»ºç›¸æœºå†…å‚çŸ©é˜µ
        camera_matrix = np.array([
            [focal_length, 0, w / 2],
            [0, focal_length, h / 2],
            [0, 0, 1]
        ], dtype=np.float32)
        
        return camera_matrix
    
    def process_video(self, 
                     frames: List[np.ndarray],
                     camera_matrix: Optional[np.ndarray] = None,
                     output_dir: str = 'output') -> Dict:
        """å¤„ç†è§†é¢‘åºåˆ—"""
        
        if len(frames) < 2:
            raise ValueError("è‡³å°‘éœ€è¦2å¸§å›¾åƒ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # å¦‚æœæ²¡æœ‰æä¾›ç›¸æœºçŸ©é˜µï¼Œä¼°è®¡ä¸€ä¸ª
        if camera_matrix is None:
            camera_matrix = self.estimate_camera_matrix(frames[0].shape)
            print("ä½¿ç”¨ä¼°è®¡çš„ç›¸æœºå†…å‚çŸ©é˜µ")
        
        print("å¼€å§‹è®¡ç®—å…‰æµ...")
        
        # è®¡ç®—æ‰€æœ‰å¸§é—´çš„å…‰æµ
        flows = []  # ç”¨äºåˆ†æçš„å…‰æµï¼ˆå¯èƒ½æ˜¯è¡¥å¿åçš„ï¼‰
        original_flows = []  # åŸå§‹å…‰æµ
        camera_compensation_results = []  # å­˜å‚¨ç›¸æœºè¡¥å¿ç»“æœ
        
        for i in tqdm(range(len(frames) - 1), desc="è®¡ç®—å…‰æµ"):
            flow = self.raft_predictor.predict_flow(frames[i], frames[i + 1])
            # ç¡®ä¿flowæ ¼å¼æ­£ç¡® (2, H, W) -> (H, W, 2)
            if flow.shape[0] == 2:
                flow = flow.transpose(1, 2, 0)  # (2, H, W) -> (H, W, 2)
            elif len(flow.shape) == 2:
                # å¦‚æœæ˜¯2Dï¼Œå¯èƒ½éœ€è¦æ·»åŠ é€šé“ç»´åº¦
                print(f"è­¦å‘Š: å¸§{i}çš„å…‰æµå½¢çŠ¶å¼‚å¸¸: {flow.shape}")
                continue
            
            original_flows.append(flow.copy())  # ä¿å­˜åŸå§‹å…‰æµ
            
            # åº”ç”¨ç›¸æœºè¡¥å¿
            if self.enable_camera_compensation and self.camera_compensator is not None:
                comp_result = self.camera_compensator.compensate(flow, frames[i], frames[i + 1])
                camera_compensation_results.append(comp_result)
                # ä½¿ç”¨æ®‹å·®å…‰æµï¼ˆè¡¥å¿åçš„å…‰æµï¼‰ç”¨äºåˆ†æ
                flows.append(comp_result['residual_flow'])
            else:
                flows.append(flow)
                camera_compensation_results.append(None)
        
        print("å¼€å§‹åˆ†æé™æ€ç‰©ä½“åŠ¨æ€åº¦...")
        
        # è®¡ç®—æ—¶åºåŠ¨æ€åº¦
        temporal_result = self.dynamics_calculator.calculate_temporal_dynamics(
            flows, frames, camera_matrix
        )
        
        # æ·»åŠ ç›¸æœºè¡¥å¿ä¿¡æ¯åˆ°ç»“æœä¸­
        temporal_result['camera_compensation_enabled'] = self.enable_camera_compensation
        temporal_result['camera_compensation_results'] = camera_compensation_results
        temporal_result['original_flows'] = original_flows  # ä¿å­˜åŸå§‹å…‰æµä¾›å¯è§†åŒ–ä½¿ç”¨
        
        # è®¡ç®—ç»Ÿä¸€åŠ¨æ€åº¦åˆ†æ•°
        print("è®¡ç®—ç»Ÿä¸€åŠ¨æ€åº¦åˆ†æ•°...")
        unified_result = self.unified_scorer.calculate_unified_score(
            temporal_result, self.enable_camera_compensation
        )
        
        # åˆ†ç±»åŠ¨æ€åº¦
        classification = self.dynamics_classifier.classify(
            unified_result['unified_dynamics_score']
        )
        
        # æ·»åŠ åˆ°ç»“æœä¸­
        temporal_result['unified_dynamics'] = unified_result
        temporal_result['dynamics_classification'] = classification
        
        # ä¿å­˜ç»“æœ
        self.save_results(temporal_result, frames, flows, output_dir)
        
        return temporal_result
    
    def save_results(self, 
                    result: Dict,
                    frames: List[np.ndarray],
                    flows: List[np.ndarray],
                    output_dir: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        
        # ä¿å­˜æ•°å€¼ç»“æœ
        numeric_result = {
            'temporal_stats': result['temporal_stats'],
            'frame_count': len(frames),
            'flow_count': len(flows),
            'camera_compensation_enabled': result.get('camera_compensation_enabled', False),
            # ç»Ÿä¸€åŠ¨æ€åº¦åˆ†æ•°ï¼ˆæ–°å¢ï¼‰
            'unified_dynamics_score': result.get('unified_dynamics', {}).get('unified_dynamics_score', None),
            'scene_type': result.get('unified_dynamics', {}).get('scene_type', None),
            'dynamics_category': result.get('dynamics_classification', {}).get('category', None),
            'dynamics_category_id': result.get('dynamics_classification', {}).get('category_id', None)
        }
        
        # æ·»åŠ ç›¸æœºè¡¥å¿ç»Ÿè®¡ä¿¡æ¯
        if result.get('camera_compensation_enabled', False):
            camera_comp_results = result.get('camera_compensation_results', [])
            comp_stats = self._calculate_camera_compensation_stats(camera_comp_results)
            numeric_result['camera_compensation_stats'] = comp_stats
        
        # æ·»åŠ æ¯å¸§çš„æ•°å€¼ç»“æœ
        numeric_result['frame_results'] = []
        for i, frame_result in enumerate(result['frame_results']):
            numeric_frame_result = {
                'frame_index': i,
                'static_dynamics': frame_result['static_dynamics'],
                'global_dynamics': frame_result['global_dynamics']
            }
            
            # æ·»åŠ ç›¸æœºè¡¥å¿ä¿¡æ¯
            if result.get('camera_compensation_enabled', False) and i < len(result.get('camera_compensation_results', [])):
                comp_result = result['camera_compensation_results'][i]
                if comp_result is not None:
                    numeric_frame_result['camera_compensation'] = {
                        'inliers': comp_result['inliers'],
                        'total_matches': comp_result['total_matches'],
                        'homography_found': comp_result['homography'] is not None
                    }
            
            numeric_result['frame_results'].append(numeric_frame_result)
        
        # ä¿å­˜JSONç»“æœ
        with open(os.path.join(output_dir, 'analysis_results.json'), 'w', encoding='utf-8') as f:
            json.dump(numeric_result, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        report = self.generate_video_report(result)
        with open(os.path.join(output_dir, 'analysis_report.txt'), 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_visualization:
            self.save_visualizations(result, frames, flows, output_dir)
        
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _calculate_camera_compensation_stats(self, camera_comp_results: List[Optional[Dict]]) -> Dict:
        """è®¡ç®—ç›¸æœºè¡¥å¿ç»Ÿè®¡ä¿¡æ¯"""
        if not camera_comp_results:
            return {}
        
        valid_results = [r for r in camera_comp_results if r is not None and r['homography'] is not None]
        
        if not valid_results:
            return {
                'success_rate': 0.0,
                'mean_inliers': 0.0,
                'mean_match_ratio': 0.0
            }
        
        total_frames = len(camera_comp_results)
        successful_frames = len(valid_results)
        
        inliers = [r['inliers'] for r in valid_results]
        match_ratios = [r['inliers'] / max(r['total_matches'], 1) for r in valid_results]
        
        return {
            'success_rate': successful_frames / total_frames,
            'successful_frames': successful_frames,
            'total_frames': total_frames,
            'mean_inliers': float(np.mean(inliers)) if inliers else 0.0,
            'std_inliers': float(np.std(inliers)) if inliers else 0.0,
            'mean_match_ratio': float(np.mean(match_ratios)) if match_ratios else 0.0,
            'std_match_ratio': float(np.std(match_ratios)) if match_ratios else 0.0
        }
    
    def save_visualizations(self, 
                           result: Dict,
                           frames: List[np.ndarray],
                           flows: List[np.ndarray],
                           output_dir: str):
        """ä¿å­˜å¯è§†åŒ–ç»“æœ"""
        
        vis_dir = os.path.join(output_dir, 'visualizations')
        os.makedirs(vis_dir, exist_ok=True)
        
        # ä¿å­˜å…³é”®å¸§çš„è¯¦ç»†åˆ†æ
        key_frames = [0, len(frames)//4, len(frames)//2, 3*len(frames)//4, len(frames)-2]
        key_frames = [i for i in key_frames if i < len(result['frame_results'])]
        
        for i in key_frames:
            frame_result = result['frame_results'][i]
            fig = self.dynamics_calculator.visualize_results(
                frames[i], flows[i], frame_result
            )
            fig.savefig(os.path.join(vis_dir, f'frame_{i:04d}_analysis.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close(fig)
        
        # ç»˜åˆ¶æ—¶åºåŠ¨æ€åº¦æ›²çº¿
        self.plot_temporal_dynamics(result, os.path.join(vis_dir, 'temporal_dynamics.png'))
        
        # ç»˜åˆ¶é™æ€åŒºåŸŸæ¯”ä¾‹å˜åŒ–
        self.plot_static_ratio_changes(result, os.path.join(vis_dir, 'static_ratio_changes.png'))
        
        # å¦‚æœå¯ç”¨äº†ç›¸æœºè¡¥å¿ï¼Œç»˜åˆ¶è¡¥å¿æ•ˆæœå¯¹æ¯”
        if result.get('camera_compensation_enabled', False):
            self.plot_camera_compensation_comparison(result, frames, os.path.join(vis_dir, 'camera_compensation_comparison.png'))
    
    def plot_temporal_dynamics(self, result: Dict, save_path: str):
        """ç»˜åˆ¶æ—¶åºåŠ¨æ€åº¦æ›²çº¿"""
        frame_results = result['frame_results']
        
        dynamics_scores = [r['static_dynamics']['dynamics_score'] for r in frame_results]
        static_ratios = [r['global_dynamics']['static_ratio'] for r in frame_results]
        consistency_scores = [r['global_dynamics']['consistency_score'] for r in frame_results]
        
        fig, axes = plt.subplots(3, 1, figsize=(12, 10))
        
        # åŠ¨æ€åº¦åˆ†æ•°
        axes[0].plot(dynamics_scores, 'b-', linewidth=2, label='Dynamics Score')
        axes[0].axhline(y=1.0, color='r', linestyle='--', alpha=0.7, label='Low Threshold')
        axes[0].axhline(y=2.0, color='orange', linestyle='--', alpha=0.7, label='High Threshold')
        axes[0].set_ylabel('Dynamics Score')
        axes[0].set_title('Static Object Dynamics Over Time')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # é™æ€åŒºåŸŸæ¯”ä¾‹
        axes[1].plot(static_ratios, 'g-', linewidth=2, label='Static Ratio')
        axes[1].axhline(y=0.7, color='r', linestyle='--', alpha=0.7, label='Good Threshold')
        axes[1].set_ylabel('Static Ratio')
        axes[1].set_title('Static Region Ratio Over Time')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # ä¸€è‡´æ€§åˆ†æ•°
        axes[2].plot(consistency_scores, 'm-', linewidth=2, label='Consistency Score')
        axes[2].set_ylabel('Consistency Score')
        axes[2].set_xlabel('Frame Index')
        axes[2].set_title('Flow Consistency Over Time')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_camera_compensation_comparison(self, result: Dict, frames: List[np.ndarray], save_path: str):
        """ç»˜åˆ¶ç›¸æœºè¡¥å¿æ•ˆæœå¯¹æ¯”"""
        original_flows = result.get('original_flows', [])
        camera_comp_results = result.get('camera_compensation_results', [])
        
        if not original_flows or not camera_comp_results:
            return
        
        # é€‰æ‹©å‡ ä¸ªå…³é”®å¸§è¿›è¡Œå¯¹æ¯”
        key_frames = [0, len(original_flows)//4, len(original_flows)//2, 3*len(original_flows)//4, len(original_flows)-1]
        key_frames = [i for i in key_frames if i < len(original_flows) and camera_comp_results[i] is not None]
        
        if not key_frames:
            return
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        n_frames = min(3, len(key_frames))  # æœ€å¤šæ˜¾ç¤º3å¸§
        fig, axes = plt.subplots(n_frames, 4, figsize=(20, 5*n_frames))
        
        if n_frames == 1:
            axes = axes.reshape(1, -1)
        
        for idx, frame_idx in enumerate(key_frames[:n_frames]):
            original_flow = original_flows[frame_idx]
            comp_result = camera_comp_results[frame_idx]
            
            # è®¡ç®—å¹…åº¦
            original_mag = np.sqrt(original_flow[:, :, 0]**2 + original_flow[:, :, 1]**2)
            camera_mag = np.sqrt(comp_result['camera_flow'][:, :, 0]**2 + comp_result['camera_flow'][:, :, 1]**2)
            residual_mag = np.sqrt(comp_result['residual_flow'][:, :, 0]**2 + comp_result['residual_flow'][:, :, 1]**2)
            
            # åŸå§‹å¸§
            axes[idx, 0].imshow(frames[frame_idx])
            axes[idx, 0].set_title(f'Frame {frame_idx}')
            axes[idx, 0].axis('off')
            
            # åŸå§‹å…‰æµ
            im1 = axes[idx, 1].imshow(original_mag, cmap='jet', vmin=0, vmax=np.percentile(original_mag, 95))
            axes[idx, 1].set_title(f'Original Flow\n(mean={original_mag.mean():.2f})')
            axes[idx, 1].axis('off')
            plt.colorbar(im1, ax=axes[idx, 1], fraction=0.046)
            
            # ç›¸æœºå…‰æµ
            im2 = axes[idx, 2].imshow(camera_mag, cmap='jet', vmin=0, vmax=np.percentile(original_mag, 95))
            axes[idx, 2].set_title(f'Camera Flow\n(inliers={comp_result["inliers"]})')
            axes[idx, 2].axis('off')
            plt.colorbar(im2, ax=axes[idx, 2], fraction=0.046)
            
            # æ®‹å·®å…‰æµ
            im3 = axes[idx, 3].imshow(residual_mag, cmap='jet', vmin=0, vmax=np.percentile(residual_mag, 95))
            axes[idx, 3].set_title(f'Residual Flow\n(mean={residual_mag.mean():.2f})')
            axes[idx, 3].axis('off')
            plt.colorbar(im3, ax=axes[idx, 3], fraction=0.046)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_static_ratio_changes(self, result: Dict, save_path: str):
        """ç»˜åˆ¶é™æ€åŒºåŸŸæ¯”ä¾‹å˜åŒ–"""
        frame_results = result['frame_results']
        
        static_ratios = [r['global_dynamics']['static_ratio'] for r in frame_results]
        dynamic_ratios = [r['global_dynamics']['dynamic_ratio'] for r in frame_results]
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        x = range(len(static_ratios))
        ax.fill_between(x, 0, static_ratios, alpha=0.7, color='green', label='Static Regions')
        ax.fill_between(x, static_ratios, 1.0, alpha=0.7, color='red', label='Dynamic Regions')
        
        ax.set_ylabel('Region Ratio')
        ax.set_xlabel('Frame Index')
        ax.set_title('Static vs Dynamic Region Ratio Over Time')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_video_report(self, result: Dict) -> str:
        """ç”Ÿæˆè§†é¢‘åˆ†ææŠ¥å‘Š"""
        temporal_stats = result['temporal_stats']
        frame_count = len(result['frame_results'])
        camera_comp_enabled = result.get('camera_compensation_enabled', False)
        
        # è·å–ç»Ÿä¸€åŠ¨æ€åº¦ä¿¡æ¯
        unified_dynamics = result.get('unified_dynamics', {})
        dynamics_class = result.get('dynamics_classification', {})
        
        report = f"""
è§†é¢‘åŠ¨æ€åº¦ç»¼åˆåˆ†ææŠ¥å‘Š
================================================

è§†é¢‘åŸºæœ¬ä¿¡æ¯:
- æ€»å¸§æ•°: {frame_count}
- åˆ†æå¸§æ•°: {len(result['frame_results'])}
- ç›¸æœºè¡¥å¿: {'å¯ç”¨' if camera_comp_enabled else 'ç¦ç”¨'}

"""
        
        # æ·»åŠ ç»Ÿä¸€åŠ¨æ€åº¦è¯„ä¼°
        if unified_dynamics:
            unified_score = unified_dynamics.get('unified_dynamics_score', 0)
            scene_type = unified_dynamics.get('scene_type', 'unknown')
            confidence = unified_dynamics.get('confidence', 0)
            
            report += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š ç»Ÿä¸€åŠ¨æ€åº¦è¯„ä¼° (Unified Dynamics Score)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ç»¼åˆåŠ¨æ€åº¦åˆ†æ•°: {unified_score:.3f} / 1.000
åœºæ™¯ç±»å‹: {scene_type}
ç½®ä¿¡åº¦: {confidence:.1%}

åˆ†ç±»ç»“æœ: {dynamics_class.get('description', 'N/A')}
å…¸å‹ä¾‹å­: {', '.join(dynamics_class.get('typical_examples', []))}

{unified_dynamics.get('interpretation', '')}

"""
        
        report += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
        
        # æ·»åŠ ç›¸æœºè¡¥å¿ç»Ÿè®¡
        if camera_comp_enabled and 'camera_compensation_results' in result:
            comp_stats = self._calculate_camera_compensation_stats(result['camera_compensation_results'])
            if comp_stats:
                report += f"""ç›¸æœºè¿åŠ¨è¡¥å¿ç»Ÿè®¡:
- æˆåŠŸç‡: {comp_stats['success_rate']:.1%} ({comp_stats['successful_frames']}/{comp_stats['total_frames']})
- å¹³å‡å†…ç‚¹æ•°: {comp_stats['mean_inliers']:.1f} Â± {comp_stats['std_inliers']:.1f}
- å¹³å‡åŒ¹é…ç‡: {comp_stats['mean_match_ratio']:.1%} Â± {comp_stats['std_match_ratio']:.1%}

"""
        
        report += f"""æ—¶åºåŠ¨æ€åº¦ç»Ÿè®¡:
- å¹³å‡åŠ¨æ€åº¦åˆ†æ•°: {temporal_stats['mean_dynamics_score']:.3f}
- åŠ¨æ€åº¦åˆ†æ•°æ ‡å‡†å·®: {temporal_stats['std_dynamics_score']:.3f}
- æœ€å¤§åŠ¨æ€åº¦åˆ†æ•°: {temporal_stats['max_dynamics_score']:.3f}
- æœ€å°åŠ¨æ€åº¦åˆ†æ•°: {temporal_stats['min_dynamics_score']:.3f}

é™æ€åŒºåŸŸåˆ†æ:
- å¹³å‡é™æ€åŒºåŸŸæ¯”ä¾‹: {temporal_stats['mean_static_ratio']:.3f}
- é™æ€åŒºåŸŸæ¯”ä¾‹æ ‡å‡†å·®: {temporal_stats['std_static_ratio']:.3f}

ä¸€è‡´æ€§åˆ†æ:
- å¹³å‡ä¸€è‡´æ€§åˆ†æ•°: {temporal_stats['mean_consistency_score']:.3f}
- æ—¶åºç¨³å®šæ€§: {temporal_stats['temporal_stability']:.3f}

ç»¼åˆè¯„ä¼°:
"""
        
        # æ·»åŠ ç»¼åˆè¯„ä¼°
        mean_dynamics = temporal_stats['mean_dynamics_score']
        mean_static_ratio = temporal_stats['mean_static_ratio']
        temporal_stability = temporal_stats['temporal_stability']
        
        if mean_dynamics < 1.0:
            report += "âœ“ é™æ€ç‰©ä½“åŠ¨æ€åº¦ä½ï¼Œç›¸æœºè¿åŠ¨è¡¥å¿æ•ˆæœè‰¯å¥½\n"
        elif mean_dynamics < 2.0:
            report += "â–³ é™æ€ç‰©ä½“åŠ¨æ€åº¦ä¸­ç­‰ï¼Œå­˜åœ¨è½»å¾®æ®‹ä½™è¿åŠ¨\n"
        else:
            report += "âœ— é™æ€ç‰©ä½“åŠ¨æ€åº¦é«˜ï¼Œå¯èƒ½å­˜åœ¨è¡¥å¿è¯¯å·®æˆ–çœŸå®ç‰©ä½“è¿åŠ¨\n"
        
        if mean_static_ratio > 0.7:
            report += "âœ“ åœºæ™¯ä¸»è¦ç”±é™æ€ç‰©ä½“ç»„æˆï¼Œé€‚åˆè¿›è¡Œé™æ€ç‰©ä½“åŠ¨æ€åº¦åˆ†æ\n"
        elif mean_static_ratio > 0.5:
            report += "â–³ åœºæ™¯ä¸­é™æ€å’ŒåŠ¨æ€åŒºåŸŸæ¯”ä¾‹é€‚ä¸­\n"
        else:
            report += "âš  åœºæ™¯ä¸­åŠ¨æ€åŒºåŸŸè¾ƒå¤šï¼Œé™æ€ç‰©ä½“åŠ¨æ€åº¦åˆ†æå¯èƒ½ä¸å¤Ÿå‡†ç¡®\n"
        
        if temporal_stability > 0.8:
            report += "âœ“ æ—¶åºç¨³å®šæ€§é«˜ï¼ŒåŠ¨æ€åº¦è®¡ç®—ç»“æœå¯é \n"
        elif temporal_stability > 0.6:
            report += "â–³ æ—¶åºç¨³å®šæ€§ä¸­ç­‰\n"
        else:
            report += "âš  æ—¶åºç¨³å®šæ€§ä½ï¼Œç»“æœå¯èƒ½å­˜åœ¨è¾ƒå¤§æ³¢åŠ¨\n"
        
        # æ·»åŠ å»ºè®®
        report += "\nå»ºè®®:\n"
        
        if mean_dynamics > 1.5:
            report += "- è€ƒè™‘è°ƒæ•´ç›¸æœºè¿åŠ¨ä¼°è®¡å‚æ•°æˆ–ä½¿ç”¨æ›´ç²¾ç¡®çš„ç›¸æœºæ ‡å®š\n"
            report += "- æ£€æŸ¥æ˜¯å¦å­˜åœ¨çœŸå®çš„ç‰©ä½“è¿åŠ¨ï¼ˆå¦‚é£å¹ã€æŒ¯åŠ¨ç­‰ï¼‰\n"
        
        if mean_static_ratio < 0.6:
            report += "- å½“å‰åœºæ™¯åŠ¨æ€å†…å®¹è¾ƒå¤šï¼Œå»ºè®®é€‰æ‹©æ›´é™æ€çš„åœºæ™¯è¿›è¡Œæµ‹è¯•\n"
        
        if temporal_stability < 0.7:
            report += "- åŠ¨æ€åº¦è®¡ç®—ç»“æœæ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®å¢åŠ æ—¶åºå¹³æ»‘æˆ–è°ƒæ•´å‚æ•°\n"
        
        return report


def load_expected_labels(label_file: str) -> Dict[str, Union[str, float]]:
    """åŠ è½½æœŸæœ›æ ‡ç­¾æ–‡ä»¶ï¼ˆç”¨äºBadCaseæ£€æµ‹ï¼‰"""
    labels = {}
    if label_file.endswith('.json'):
        with open(label_file, 'r', encoding='utf-8') as f:
            labels = json.load(f)
    return labels


def process_single_video(processor, video_path, output_dir, camera_fov, expected_label=None):
    """
    å¤„ç†å•ä¸ªè§†é¢‘
    
    Args:
        processor: VideoProcessorå®ä¾‹
        video_path: è§†é¢‘è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        camera_fov: ç›¸æœºè§†åœºè§’
        expected_label: æœŸæœ›æ ‡ç­¾ï¼ˆå¯é€‰ï¼Œç”¨äºBadCaseæ£€æµ‹ï¼‰
    """
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    video_output_dir = os.path.join(output_dir, video_name)
    
    print(f"\n{'='*70}")
    print(f"å¤„ç†è§†é¢‘: {video_name}")
    if expected_label is not None:
        print(f"æœŸæœ›æ ‡ç­¾: {expected_label}")
    print(f"{'='*70}")
    
    try:
        # åŠ è½½è§†é¢‘
        frames = processor.load_video(video_path)
        
        # ä¼°è®¡ç›¸æœºå†…å‚
        camera_matrix = processor.estimate_camera_matrix(frames[0].shape, camera_fov)
        
        # å¤„ç†è§†é¢‘
        result = processor.process_video(frames, camera_matrix, video_output_dir)
        
        # æå–åŸºç¡€ä¿¡æ¯
        temporal_stats = result['temporal_stats']
        unified = result.get('unified_dynamics', {})
        actual_score = unified.get('unified_dynamics_score', 0.5)
        confidence = unified.get('confidence', 0.0)
        
        # æ„å»ºåŸºç¡€è¿”å›ç»“æœ
        video_result = {
            'video_name': video_name,
            'video_path': video_path,
            'status': 'success',
            'frame_count': len(frames),
            'mean_dynamics_score': temporal_stats['mean_dynamics_score'],
            'mean_static_ratio': temporal_stats['mean_static_ratio'],
            'temporal_stability': temporal_stats['temporal_stability'],
            'actual_score': actual_score,
            'confidence': confidence,
            'output_dir': video_output_dir,
            'full_result': result
        }
        
        # å¦‚æœæä¾›äº†æœŸæœ›æ ‡ç­¾ï¼Œè¿›è¡ŒBadCaseæ£€æµ‹
        if expected_label is not None:
            badcase_result = processor.badcase_analyzer.analyze_with_details(
                result, expected_label
            )
            
            result['badcase_detection'] = badcase_result
            video_result['expected_label'] = expected_label
            video_result['is_badcase'] = badcase_result['is_badcase']
            video_result['badcase_type'] = badcase_result.get('badcase_type', 'normal')
            video_result['severity'] = badcase_result.get('severity', 'normal')
            video_result['mismatch_score'] = badcase_result.get('mismatch_score', 0.0)
            
            # ä¿å­˜BadCaseæ£€æµ‹ç»“æœ
            badcase_report_path = os.path.join(video_output_dir, 'badcase_report.txt')
            with open(badcase_report_path, 'w', encoding='utf-8') as f:
                f.write(f"è§†é¢‘: {video_name}\n")
                f.write(f"æœŸæœ›æ ‡ç­¾: {expected_label}\n")
                f.write(f"å®é™…åŠ¨æ€åº¦: {actual_score:.3f}\n")
                f.write(f"æ˜¯å¦BadCase: {'æ˜¯' if badcase_result['is_badcase'] else 'å¦'}\n")
                if badcase_result['is_badcase']:
                    f.write(f"\n{badcase_result['description']}\n")
                    f.write(f"\n{badcase_result['suggestion']}\n")
                    if 'diagnosis' in badcase_result:
                        f.write(f"\næ ¹å› è¯Šæ–­:\n")
                        f.write(f"  ä¸»è¦é—®é¢˜: {badcase_result['diagnosis']['primary_issue']}\n")
                        f.write(f"  è´¡çŒ®å› ç´ :\n")
                        for factor in badcase_result['diagnosis']['contributing_factors']:
                            f.write(f"    - {factor}\n")
            
            # æ‰“å°BadCaseç»“æœ
            if badcase_result['is_badcase']:
                print(f"âš   BadCaseæ£€æµ‹: æ˜¯")
                print(f"   ç±»å‹: {badcase_result['badcase_type']}")
                print(f"   ä¸¥é‡ç¨‹åº¦: {badcase_result['severity']}")
                print(f"   ä¸åŒ¹é…åº¦: {badcase_result['mismatch_score']:.3f}")
            else:
                print(f"âœ“  è´¨é‡æ­£å¸¸")
        
        return video_result
        
    except Exception as e:
        print(f"é”™è¯¯: å¤„ç†è§†é¢‘å¤±è´¥ - {e}")
        return {
            'video_name': video_name,
            'video_path': video_path,
            'status': 'failed',
            'error': str(e),
            'is_badcase': None if expected_label is not None else None
        }


def batch_process_videos(processor, input_dir, output_dir, camera_fov, badcase_labels=None):
    """
    æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰è§†é¢‘
    
    Args:
        processor: VideoProcessorå®ä¾‹
        input_dir: è§†é¢‘ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        camera_fov: ç›¸æœºè§†åœºè§’
        badcase_labels: å¯é€‰ï¼ŒæœŸæœ›æ ‡ç­¾å­—å…¸ï¼Œå¯ç”¨BadCaseæ£€æµ‹
    """
    # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.flv', '*.wmv']
    video_files = []
    for ext in video_extensions:
        video_files.extend(glob.glob(os.path.join(input_dir, ext)))
        video_files.extend(glob.glob(os.path.join(input_dir, ext.upper())))
    
    # å»é‡ï¼ˆWindowsç³»ç»Ÿä¸‹å¤§å°å†™ä¸æ•æ„Ÿä¼šå¯¼è‡´é‡å¤ï¼‰
    video_files = list(set(video_files))
    
    if not video_files:
        print(f"é”™è¯¯: åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        return []
    
    video_files.sort()
    print(f"\næ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
    if badcase_labels:
        print(f"å·²æœ‰ {len(badcase_labels)} ä¸ªè§†é¢‘æ ‡ç­¾")
    
    # å¤„ç†æ¯ä¸ªè§†é¢‘
    results = []
    for i, video_path in enumerate(video_files, 1):
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        
        # è·å–æœŸæœ›æ ‡ç­¾ï¼ˆå¦‚æœå¯ç”¨BadCaseæ¨¡å¼ï¼‰
        expected_label = None
        if badcase_labels is not None:
            expected_label = badcase_labels.get(video_name)
            if expected_label is None:
                print(f"\nâš   è§†é¢‘ {video_name} æ— æœŸæœ›æ ‡ç­¾ï¼Œè·³è¿‡BadCaseæ£€æµ‹")
                expected_label = 'dynamic'  # é»˜è®¤æœŸæœ›
        
        print(f"\nè¿›åº¦: {i}/{len(video_files)}")
        result = process_single_video(processor, video_path, output_dir, camera_fov, expected_label)
        results.append(result)
    
    # ä¿å­˜æ‰¹é‡å¤„ç†æ€»ç»“
    if badcase_labels is not None:
        # BadCaseæŠ¥å‘Š
        batch_summary = processor.badcase_analyzer.generate_batch_summary(results)
        processor.badcase_analyzer.save_batch_report(batch_summary, results, output_dir)
        return batch_summary
    else:
        # æ™®é€šæŠ¥å‘Š
        save_batch_summary(results, output_dir)
        return results


def save_batch_summary(results, output_dir):
    """ä¿å­˜æ‰¹é‡å¤„ç†æ€»ç»“"""
    summary_path = os.path.join(output_dir, 'batch_summary.txt')
    
    # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥æ•°é‡
    success_count = sum(1 for r in results if r['status'] == 'success')
    failed_count = len(results) - success_count
    
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("æ‰¹é‡è§†é¢‘å¤„ç†æ€»ç»“\n")
        f.write("=" * 70 + "\n\n")
        
        f.write(f"æ€»è§†é¢‘æ•°: {len(results)}\n")
        f.write(f"æˆåŠŸå¤„ç†: {success_count}\n")
        f.write(f"å¤„ç†å¤±è´¥: {failed_count}\n\n")
        
        f.write("=" * 70 + "\n")
        f.write("è¯¦ç»†ç»“æœ\n")
        f.write("=" * 70 + "\n\n")
        
        for result in results:
            f.write(f"è§†é¢‘: {result['video_name']}\n")
            if result['status'] == 'success':
                f.write(f"  çŠ¶æ€: âœ“ æˆåŠŸ\n")
                f.write(f"  å¸§æ•°: {result['frame_count']}\n")
                f.write(f"  å¹³å‡åŠ¨æ€åº¦åˆ†æ•°: {result['mean_dynamics_score']:.3f}\n")
                f.write(f"  å¹³å‡é™æ€åŒºåŸŸæ¯”ä¾‹: {result['mean_static_ratio']:.3f}\n")
                f.write(f"  æ—¶åºç¨³å®šæ€§: {result['temporal_stability']:.3f}\n")
                f.write(f"  è¾“å‡ºç›®å½•: {result['output_dir']}\n")
            else:
                f.write(f"  çŠ¶æ€: âœ— å¤±è´¥\n")
                f.write(f"  é”™è¯¯: {result['error']}\n")
            f.write("\n")
    
    # åŒæ—¶ä¿å­˜JSONæ ¼å¼
    summary_json_path = os.path.join(output_dir, 'batch_summary.json')
    with open(summary_json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\næ‰¹é‡å¤„ç†æ€»ç»“å·²ä¿å­˜åˆ°: {summary_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é™æ€ç‰©ä½“åŠ¨æ€åº¦åˆ†æ')
    parser.add_argument('--input', '-i', required=True, 
                       help='è¾“å…¥è§†é¢‘æ–‡ä»¶ã€å›¾åƒç›®å½•æˆ–è§†é¢‘ç›®å½•è·¯å¾„ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰')
    parser.add_argument('--output', '-o', default='output',
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--raft_model', '-m', default="pretrained_models/raft-things.pth",
                       help='RAFTæ¨¡å‹è·¯å¾„')
    parser.add_argument('--max_frames', type=int, default=None,
                       help='æœ€å¤§å¤„ç†å¸§æ•°')
    parser.add_argument('--frame_skip', type=int, default=1,
                       help='å¸§è·³è·ƒé—´éš”')
    parser.add_argument('--device', default='cuda',
                       help='è®¡ç®—è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--fov', type=float, default=60.0,
                       help='ç›¸æœºè§†åœºè§’ (åº¦)')
    parser.add_argument('--batch', action='store_true',
                       help='æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šå¤„ç†è¾“å…¥ç›®å½•ä¸‹çš„æ‰€æœ‰è§†é¢‘')
    
    # BadCaseæ£€æµ‹å‚æ•°ï¼ˆå¯é€‰ï¼‰
    parser.add_argument('--badcase-labels', '-l', default=None,
                       help='æœŸæœ›æ ‡ç­¾æ–‡ä»¶ï¼ˆJSONï¼‰ï¼Œå¯ç”¨BadCaseæ£€æµ‹')
    parser.add_argument('--mismatch-threshold', type=float, default=0.3,
                       help='BadCaseä¸åŒ¹é…é˜ˆå€¼ï¼ˆé»˜è®¤0.3ï¼‰')
    
    # å¯è§†åŒ–å‚æ•°
    parser.add_argument('--visualize', dest='visualize', action='store_true',
                       help='ç¦ç”¨å¯è§†åŒ–ç”Ÿæˆï¼ˆåŠ å¿«å¤„ç†é€Ÿåº¦ï¼‰')
    parser.add_argument('--no-camera-compensation', dest='camera_compensation', action='store_false',
                       help='ç¦ç”¨ç›¸æœºè¡¥å¿ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--camera-ransac-thresh', type=float, default=1.0,
                       help='ç›¸æœºè¡¥å¿RANSACé˜ˆå€¼ï¼ˆåƒç´ ï¼‰')
    parser.add_argument('--camera-max-features', type=int, default=2000,
                       help='ç›¸æœºè¡¥å¿æœ€å¤§ç‰¹å¾ç‚¹æ•°')
    parser.add_argument('--normalize-by-resolution', dest='use_normalized_flow',
                       action='store_true',
                       help='æŒ‰åˆ†è¾¨ç‡å½’ä¸€åŒ–å…‰æµï¼ˆæ¨èå¼€å¯ä»¥ä¿è¯ä¸åŒåˆ†è¾¨ç‡è§†é¢‘çš„å…¬å¹³æ€§ï¼‰')
    parser.add_argument('--flow-threshold-ratio', type=float, default=0.002,
                       help='å½’ä¸€åŒ–åçš„é™æ€é˜ˆå€¼ï¼ˆç›¸å¯¹äºå›¾åƒå¯¹è§’çº¿ï¼Œé»˜è®¤0.002ï¼‰')
    parser.set_defaults(visualize=False, camera_compensation=True, use_normalized_flow=False)
    
    args = parser.parse_args()
    
    # åŠ è½½BadCaseæ ‡ç­¾ï¼ˆå¦‚æœæä¾›ï¼‰
    badcase_labels = None
    if args.badcase_labels:
        print(f"åŠ è½½æœŸæœ›æ ‡ç­¾: {args.badcase_labels}")
        badcase_labels = load_expected_labels(args.badcase_labels)
        print(f"å·²åŠ è½½ {len(badcase_labels)} ä¸ªæ ‡ç­¾")
    
    # å‡†å¤‡ç›¸æœºè¡¥å¿å‚æ•°
    camera_compensation_params = {
        'ransac_thresh': args.camera_ransac_thresh,
        'max_features': args.camera_max_features
    }
    
    # åˆ›å»ºè§†é¢‘å¤„ç†å™¨
    processor = VideoProcessor(
        raft_model_path=args.raft_model,
        device=args.device,
        max_frames=args.max_frames,
        frame_skip=args.frame_skip,
        enable_visualization=args.visualize,
        enable_camera_compensation=args.camera_compensation,
        camera_compensation_params=camera_compensation_params,
        use_normalized_flow=args.use_normalized_flow,
        flow_threshold_ratio=args.flow_threshold_ratio
    )
    
    # è®¾ç½®BadCaseæ£€æµ‹å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if badcase_labels:
        processor.badcase_detector = BadCaseDetector(
            mismatch_threshold=args.mismatch_threshold
        )
        processor.badcase_analyzer.detector = processor.badcase_detector
    
    # æ‰¹é‡å¤„ç†æ¨¡å¼
    if args.batch:
        if not os.path.isdir(args.input):
            raise ValueError(f"æ‰¹é‡æ¨¡å¼éœ€è¦æä¾›ç›®å½•è·¯å¾„: {args.input}")
        
        results = batch_process_videos(processor, args.input, args.output, args.fov, badcase_labels)
        
        # æ‰“å°æ€»ç»“
        print(f"\n{'='*70}")
        print(f"æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"{'='*70}")
        
        if badcase_labels:
            # BadCaseæ¨¡å¼æ€»ç»“
            print(f"æ€»è§†é¢‘æ•°: {results['total_videos']}")
            print(f"æˆåŠŸå¤„ç†: {results['successful']}")
            print(f"BadCaseæ•°é‡: {results['badcase_count']}")
            print(f"BadCaseæ¯”ä¾‹: {results['badcase_rate']:.1%}")
            print(f"\nä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
            for severity, count in results['severity_distribution'].items():
                print(f"  {severity}: {count}")
        else:
            # æ™®é€šæ¨¡å¼æ€»ç»“
            success_count = sum(1 for r in results if r['status'] == 'success')
            print(f"æ€»è®¡: {len(results)} ä¸ªè§†é¢‘")
            print(f"æˆåŠŸ: {success_count} ä¸ª")
            print(f"å¤±è´¥: {len(results) - success_count} ä¸ª")
        
        print(f"\nç»“æœä¿å­˜åˆ°: {args.output}")
        print(f"{'='*70}")
        
    # å•ä¸ªè§†é¢‘/å›¾åƒç›®å½•å¤„ç†æ¨¡å¼
    else:
        # åŠ è½½è§†é¢‘æˆ–å›¾åƒ
        if os.path.isfile(args.input):
            # å•ä¸ªè§†é¢‘æ–‡ä»¶
            frames = processor.load_video(args.input)
        elif os.path.isdir(args.input):
            # å›¾åƒåºåˆ—ç›®å½•
            frames = processor.extract_frames_from_images(args.input)
            if len(frames) == 0:
                raise ValueError(
                    f"é”™è¯¯ï¼šç›®å½• '{args.input}' ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶ï¼\n"
                    f"æç¤ºï¼š\n"
                    f"  - å¦‚æœè¦å¤„ç†å•ä¸ªè§†é¢‘ï¼Œè¯·ç›´æ¥æŒ‡å®šè§†é¢‘æ–‡ä»¶è·¯å¾„\n"
                    f"  - å¦‚æœè¦æ‰¹é‡å¤„ç†è§†é¢‘ç›®å½•ï¼Œè¯·æ·»åŠ  --batch å‚æ•°\n"
                    f"  - å¦‚æœæ˜¯å›¾åƒåºåˆ—ï¼Œè¯·ç¡®ä¿ç›®å½•åŒ…å« .jpg/.png ç­‰å›¾åƒæ–‡ä»¶"
                )
        else:
            raise ValueError(f"è¾“å…¥è·¯å¾„æ— æ•ˆ: {args.input}")
        
        # ä¼°è®¡ç›¸æœºå†…å‚
        camera_matrix = processor.estimate_camera_matrix(frames[0].shape, args.fov)
        
        # å¤„ç†è§†é¢‘
        result = processor.process_video(frames, camera_matrix, args.output)
        
        # æ‰“å°ç®€è¦ç»“æœ
        temporal_stats = result['temporal_stats']
        print(f"\nåˆ†æå®Œæˆ!")
        print(f"å¹³å‡é™æ€ç‰©ä½“åŠ¨æ€åº¦åˆ†æ•°: {temporal_stats['mean_dynamics_score']:.3f}")
        print(f"å¹³å‡é™æ€åŒºåŸŸæ¯”ä¾‹: {temporal_stats['mean_static_ratio']:.3f}")
        print(f"æ—¶åºç¨³å®šæ€§: {temporal_stats['temporal_stability']:.3f}")
        print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == '__main__':
    main()